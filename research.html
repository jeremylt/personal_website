<!DOCTYPE html>

<html lang="en" data-content_root="./">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Research Statement</title>
    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=5ecbeea2" />
    <link rel="stylesheet" type="text/css" href="_static/basic.css?v=b08954a9" />
    <link rel="stylesheet" type="text/css" href="_static/alabaster.css?v=d5a349cf" />
    <link rel="stylesheet" type="text/css" href="_static/css/custom.css?v=d9c14bda" />
    <script src="_static/documentation_options.js?v=5929fcd5"></script>
    <script src="_static/doctools.js?v=fd6eb6e6"></script>
    <script src="_static/sphinx_highlight.js?v=6ffebe34"></script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@4/tex-mml-chtml.js"></script>
    <script defer="defer" src="https://kit.fontawesome.com/4cef94740e.js"></script>
    <link rel="icon" href="_static/Icon.ico"/>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Teaching Statement" href="teaching.html" />
    <link rel="prev" title="Software" href="software.html" />
   
  <link rel="stylesheet" href="_static/custom.css" type="text/css" />
  

  
  

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
          

          <div class="body" role="main">
            
  <section id="research-statement">
<h1>Research Statement<a class="headerlink" href="#research-statement" title="Link to this heading">¶</a></h1>
<section id="open-source-scientific-software-for-modern-hpc-hardware">
<h2>Open Source Scientific Software for Modern HPC Hardware<a class="headerlink" href="#open-source-scientific-software-for-modern-hpc-hardware" title="Link to this heading">¶</a></h2>
<p>Highly accurate physics based numerical simulations are important in a wide range of modern science and engineering applications.
These simulations allow us to investigate many different scenarios where it is impractical or unsafe to conduct a large number of physical experiments or act as a prelude to a more limited and focused set of experiments.
However, high fidelity simulations require a large amount of computational power.
If these simulations are being used in uncertainty quantification or optimization workflows, then the amount resources required is amplified.
It is important to innovate algorithmically to best use modern computing resources in support of our research goals.</p>
<p>Many fluid dynamics and solid mechanics applications use finite-element like discretizations of the physically based PDEs in simulations modeling behavior of interest.
High-order matrix-free finite element-like operators on elements with tensor-product bases offer superior performance on modern high performance computing (HPC) hardware when compared to the standard industry approach of assembled sparse matrices, both with respect to the number of floating point operations needed for operator evaluation and the memory transfer needed for a matrix-vector product.
However, matrix-free operators require iterative solvers, such as Krylov subspace methods, and these iterative solvers converge slowly for high-order operators because these operators are increasingly ill-conditioned as polynomial order of the bases increases.
Preconditioning techniques can significantly improve the convergence of these iterative solvers for high-order matrix-free finite element operators.</p>
<p>In my research, I focus on using high-order matrix-free methods with appropriate preconditioners to achieve high performance on modern HPC hardware.
I build core computational infrastructure and preconditioners to support a variety of application areas.
To this end, my research efforts are centered around building open source software packages while enabling and accelerating the research of students and researchers who are using these packages.
This works allows us to complete simulations in hours or days that could take weeks to complete with older approaches.</p>
</section>
<section id="high-order-matrix-free">
<h2>High-Order Matrix-Free<a class="headerlink" href="#high-order-matrix-free" title="Link to this heading">¶</a></h2>
<p>In order to innovate algorithmically and best use modern HPC resources, we must understand the constraints of the hardware.
Two key performance metrics for HPC hardware are Floating Point Operations per Second (FLOPs) and memory and network bandwidth.
FLOPs is the more widely popularized of these two metrics, but memory and network bandwidth is a common bottleneck in HPC application codes.
As discussed in McCalpin’s Supercomputing 2016 invited talk <span id="id1">[<a class="reference internal" href="#id30" title="John D McCalpin. Memory bandwidth and system balance in HPC systems. Invited talk, Supercomputing, 2016.">McC16</a>]</span>, the maximum FLOP rates for new HPC hardware have improved approximately twice as much as memory and network bandwidth, for both CPUs and GPUs.
The ratio of FLOPs to memory bandwidth required for high-order matrix-free finite element-like operators on elements with tensor-product bases is closer to the capabilities of modern HPC hardware.</p>
<p>The maximum FLOP rate given by the Top 500 <span id="id2">[<a class="reference internal" href="#id31" title="Hans Meuer, Erich Strohmaier, Jack Dongarra, Horst Simon, and Martin Meuer. Top 500 list. 2020. URL: http://www.top500.org/.">MSD+20</a>]</span> list is measured by High-Performance Linpack (HPL) <span id="id3">[<a class="reference internal" href="#id32" title="Antoine Petitet, R Clint Whaley, Jack Dongarra, and Andrew Cleary. HPL-a portable implementation of the high-performance Linpack benchmark for distributed-memory computers. 2004. URL: http://www.netlib.org/benchmark/hpl/.">PWDC04</a>]</span>.
High-Performance Geometric Multigrid (HPGMG) <span id="id4">[<a class="reference internal" href="#id34" title="Mark Adams, Jed Brown, John Shalf, Brian Straalen, Erich Strohmaier, and Samuel Williams. HPGMG 1.0: a benchmark for ranking high performance computing systems. LBNL Technical Report, pages, 05 2014.">ABS+14</a>]</span> and High-Performance Conjugate Gradient (HPCG) <span id="id5">[<a class="reference internal" href="#id33" title="Jack Dongarra, Michael A Heroux, and Piotr Luszczek. High-performance conjugate-gradient benchmark. International Journal of High Performance Computing Applications, 30(1):3–10, 2016.">DHL16</a>]</span> measure performance with a benchmark problem that is more representative of scientific simulations.
Conventional sparse matrix implementations can struggle to achieve more than 1% of the maximum FLOP rate due to their inefficient usage of the limited memory bandwidth, which highlights the need for matrix-free implementations.</p>
<p>High-order finite elements also offer the high accuracy needed for modern simulations as well as exponential convergence for sufficiently smooth problems.
While the smoothness of the solution for practical problems may prevent exponential convergence, high-order finite elements still offer convergence that is no worse than a comparable low-order mesh with a larger number of elements.
For discussion of the convergence of high-order methods, see <span id="id6">[<a class="reference internal" href="#id100" title="Ivo Babuška and Manil Suri. The p and h-p versions of the finite element method, basic principles and properties. SIAM review, 36(4):578–632, 1994.">BabuvskaS94</a>, <a class="reference internal" href="#id99" title="Ivo Babuška and Barna Szabo. On the rates of convergence of the finite element method. International Journal for Numerical Methods in Engineering, 18(3):323–341, 1982.">BabuvskaS82</a>, <a class="reference internal" href="#id101" title="B Guo and Ivo Babuška. The hp version of the finite element method. Computational Mechanics, 1(1):21–41, 1986.">GBabuvska86</a>]</span>, among others.</p>
<p>Although the benefits of high-order matrix-free implementations on modern HPC hardware are well understood, a significant amount of software development and research can be required for the specific model and preconditioners needed in a simulation.
For example, matrix-free implementations can expose numerical stability issues in alternate formulations of the same underlying physics <span id="id7">[<a class="reference internal" href="#id43" title="Rezgar Shakeri, Leila Ghaffari, Jeremy Thompson, and Jed Brown. Stable numerics for finite-strain elasticity. International Journal for Numerical Methods in Engineering, pages e7563, 2024. URL: https://onlinelibrary.wiley.com/doi/abs/10.1002/nme.7563, arXiv:https://onlinelibrary.wiley.com/doi/pdf/10.1002/nme.7563, doi:10.1002/nme.7563.">SGTB24</a>]</span> and the preconditioners may require parameter tuning or novel implementations.</p>
</section>
<section id="open-source-scientific-software">
<h2>Open Source Scientific Software<a class="headerlink" href="#open-source-scientific-software" title="Link to this heading">¶</a></h2>
<p>Transparency and reproducibility are the lifeblood of scientific and software advancement.
I strive to make all of my software open source and freely available while utilizing best practices for modern software development, such as documentation and continuous integration testing.
Closed source or lower quality software inhibits reproducibility and can slow research expanding upon previous work.</p>
<p>Implementations of the high-order matrix-free finite element-like operators can be found in the <a class="reference external" href="https://www.github.com/CEED/libCEED">libCEED GitHub repository</a>, along with fluid dynamics and solid mechanics mini-applications.
libCEED <span id="id8">[<a class="reference internal" href="#id102" title="Ahmad Abdelfattah, Valeria Barra, Natalie Beams, Jed Brown, Jean-Sylvain Camier, Veselin Dobrev, Yohann Dudouit, Leila Ghaffari, Tzanio Kolev, David Medina, Thilina Rathnayake, Jeremy L Thompson, and Stanimire Tomov. LibCEED User Manual. July 2021. URL: https://doi.org/10.5281/zenodo.4302737, doi:10.5281/zenodo.4302737.">ABB+21</a>, <a class="reference internal" href="#id103" title="Jed Brown, Ahmad Abdelfattah, Valeria Barra, Natalie Beams, Jean Sylvain Camier, Veselin Dobrev, Yohann Dudouit, Leila Ghaffari, Tzanio Kolev, David Medina, Will Pazner, Thilina Ratnayaka, Jeremy Thompson, and Stan Tomov. libCEED: fast algebra for high-order element-based discretizations. Journal of Open Source Software, 6(63):2945, 2021. doi:10.21105/joss.02945.">BAB+21</a>]</span> is a low-level library for the efficient high-order discretization methods developed by the ECP co-design Center for Efficient Exascale Discretizations (CEED).
LibCEED has multiple backends that can be selected at runtime, and these backends target CPU architectures <span id="id9">[<a class="reference internal" href="#id110" title="LIBXSMM. https://github.com/hfp/libxsmm, 2021.">lib21</a>]</span>, NVIDIA GPUs <span id="id10">[<a class="reference internal" href="#id108" title="Cuda. 2021. URL: https://developer.nvidia.com/about-cuda.">CUD21</a>]</span>, AMD GPUs <span id="id11">[<a class="reference internal" href="#id109" title="Hip. 2021. URL: https://rocmdocs.amd.com/en/latest/Programming_Guides/HIP-GUIDE.html.">HIP21</a>]</span>, and Intel GPUs.</p>
<p>LibCEED is based around separate hardware specific implementations of the API being selectable at runtime.
As lead maintainer, I provide new features for my research and code review and features to accelerate the work of other researchers.
The gap between a CPU and GPU implementation can be difficult to bridge for many HPC applications; to this end I developed a libCEED CPU backend implementation that replicates many of the common issues discovered while porting CPU code to GPU implementations, such as memory synchronization and dual memory spaces representing host and device memory, helping contributors and collaborators more quickly identify errors in their codes when moving to GPU implementations.</p>
<p>An excessive number of kernel launches and intermediate data structures can slow GPU performance, so I have refactored and expanded the code generation backends for GPU hardware.
While metaprogramming reduces the runtime of our simulations and allows us to put significantly larger portions of the problem on a single device, writing software that writes source code for JiT is inherently more complex to reason about.
As a result, I focus on code consistency, clarity, and documentation so that it is easier for contributors to add new GPU implementations.</p>
<p>These simulations rely upon the the linear and nonlinear solver and preconditioning infrastructure found in <a class="reference external" href="https://www.mcs.anl.gov/petsc/">PETSc</a> <span id="id12">[<a class="reference internal" href="#id104" title="Satish Balay, Shrirang Abhyankar, Mark F Adams, Jed Brown, Peter Brune, Kris Buschelman, Lisandro Dalcin, Alp Dener, Victor Eijkhout, William D Gropp, Dmitry Karpeyev, Dinesh Kaushik, Matthew G Knepley, Dave A May, Lois Curfman McInnes, Richard Tran Mills, Todd Munson, Karl Rupp, Patrick Sanan, Barry F Smith, Stefano Zampini, Hong Zhang, and Hong Zhang. PETSc users manual. Technical Report ANL-95/11 - Revision 3.15, Argonne National Laboratory, 2021. URL: https://www.mcs.anl.gov/petsc.">BAA+21</a>]</span>, the Portable, Extensible Toolkit for Scientific Computation.
PETSc is a suite of data structures and routines for the scalable, parallel solution of scientific applications modeled by partial differential equations.
As libCEED’s lead developer, I make contributions to PETSc to help ensure compatibility between libCEED and PETSc.</p>
<p><a class="reference external" href="https://gitlab.com/phypid/honee">HONEE</a> (High-Order Navier-stokes Equation Evaluator) is a fluids dyamics library based on libCEED and PETSc with a particular focus on supporting Intel GPUs.
HONEE uses the Navier-Stokes equations <span id="id13">[<a class="reference internal" href="#id119" title="Farzin Shakib, Thomas JR Hughes, and Zdeněk Johan. A new finite element formulation for computational fluid dynamics: X. the compressible Euler and Navier-Stokes equations. Computer Methods in Applied Mechanics and Engineering, 89(1-3):141–219, 1991. doi:10.1016/0045-7825(91)90041-4.">SHJ91</a>]</span> with continuous-Galerkin stabilized finite element methods, namely SUPG <span id="id14">[<a class="reference internal" href="#id120" title="Thomas J R Hughes, Guglielmo Scovazzi, and Tayfun E Tezduyar. Stabilized methods for compressible flows. Journal of Scientific Computing, 43:343–368, 2010. doi:10.1007/s10915-008-9233-5.">HST10</a>]</span>, focusing on scale-resolving simulations.
Effort is made to maintain flexibility in state variable choice, boundary conditions, time integration scheme (both implicit and explicit).
I developed the original libCEED fluid dynamics mini-app that HONEE was based on and maintain and expand core infrastructure in libCEED to support HONEE.</p>
<p><a class="reference external" href="https://gitlab.com/micromorph/ratel">Ratel</a> is a solid mechanics library that provides material models and boundary conditions implemented using libCEED and PETSc.
Ratel supports both finite element (FEM) and implicit material point method (iMPM) <span id="id15">[<a class="reference internal" href="#id117" title="William M. Coombs, Charles E. Augarde, Andrew J. Brennan, Michael J. Brown, Tim J. Charlton, Jonathan A. Knappett, Yousef Ghaffari Motlagh, and Lei Wang. On lagrangian mechanics and the implicit material point method for large deformation elasto-plasticity. Computer Methods in Applied Mechanics and Engineering, 358:112622, January 2020. doi:10.1016/j.cma.2019.112622.">CAB+20</a>, <a class="reference internal" href="#id118" title="Louis Moresi, Frédéric Dufour, and H-B Mühlhaus. A lagrangian integration point finite element method for large deformation modeling of viscoelastic geomaterials. Journal of computational physics, 184(2):476–497, 2003. doi:10.1016/S0021-9991(02)00031-1.">MDMuhlhaus03</a>]</span> simulations; with users being able to compare output for both methods with supported models.
Ratel’s material model library includes finite-strain hyperelastic, elastoplastic, viscoelastic, poroelastic, and fracture models, including stable mixed formulations for near-incompressible regimes.
Ratel users can take advantage of the packages and algorithms supported by PETSc, including Hypre <span id="id16">[<a class="reference internal" href="#id105" title="Robert D Falgout, Ruipeng Li, Björn Sjögreen, Lu Wang, and Ulrike Meier Yang. Porting hypre to heterogeneous computer architectures: strategies and experiences. Parallel Computing, 108:102840, 2021. doi:10.1016/j.parco.2021.102840.">FLSjogreen+21</a>]</span> and Kokkos <span id="id17">[<a class="reference internal" href="#id106" title="Christian R. Trott, Damien Lebrun-Grandié, Daniel Arndt, Jan Ciesko, Vinh Dang, Nathan Ellingwood, Rahulkumar Gayatri, Evan Harvey, Daisy S. Hollman, Dan Ibanez, Nevin Liber, Jonathan Madsen, Jeff Miles, David Poliakoff, Amy Powell, Sivasankaran Rajamanickam, Mikael Simberg, Dan Sunderland, Bruno Turcksin, and Jeremiah Wilke. Kokkos 3: programming model extensions for the exascale era. IEEE Transactions on Parallel and Distributed Systems, 33(4):805-817, 2022. doi:10.1109/TPDS.2021.3097283.">TLGA+22</a>]</span>, highlighting the benefits of leveraging open source software in research applications.</p>
<p>As the architect for Ratel, I work with the researchers and students implementing and using the material models to ensure the software best supports ongoing research.
Development of iMPM models is a particularly innovative feature, as matrix-free implicit MPM on GPU hardware for CU Boulder’s PSAAP Multidisciplinary Simulation Center offers an ability to run simulations with this technology significantly faster than other currently available software packages, allowing a larger number of runs to be completed and incorporated into more complex analysis of the results, such as uncertainty quantification.
Also, preconditioning for iMPM operators offers unique challenges compared to FEM operators and is a particularly rich area for research.</p>
<p>All of these software efforts allow me to work with a wide range of contributors and support the work of an even larger range of collaborators.
Additionally, while code review is designed to strengthen the end quality of software products, it also provides a natural vehicle for mentoring students in research practices.</p>
</section>
<section id="preconditioning">
<h2>Preconditioning<a class="headerlink" href="#preconditioning" title="Link to this heading">¶</a></h2>
<p>The iteration count to reach convergence for Krylov subspace methods is based, in part, upon condition number of the operator <span id="id18">[<a class="reference internal" href="#id46" title="David G Luenberger. Introduction to linear and nonlinear programming. Volume 28. Addison-Wesley Reading, MA, 1973.">Lue73</a>]</span>, and high-order finite element operators have notoriously poor condition numbers <span id="id19">[<a class="reference internal" href="#id47" title="Ning Hu, Xian-Zhong Guo, and I Norman Katz. Bounds for eigenvalues and condition numbers in the p-version of the finite element method. Mathematics of Computation, 67(224):1423–1450, 1998.">HGK98</a>]</span>.
Preconditioners help control the condition number of high-order finite elements implemented in a matrix-free fashion and therefore reduce total iteration count and total time to solution for these operators.</p>
<p>Multigrid methods are popular multi-level techniques that provide resolution independent convergence rates.
<span class="math notranslate nohighlight">\(p\)</span>-type multigrid, developed by Ronquist and Patera <span id="id20">[<a class="reference internal" href="#id51" title="Einar M Rønquist and Anthony T Patera. Spectral element multigrid. I. formulation and numerical results. Journal of Scientific Computing, 2(4):389–406, 1987.">RonquistP87</a>]</span>, is a natural choice for high-order finite elements on an unstructured mesh, can be implemented in a matrix-free fashion, and can offer more flexibility than <span class="math notranslate nohighlight">\(h\)</span>-multigrid on meshes for complex problems.</p>
<p>Local Fourier Analysis (LFA) provides a tool to predict the convergence of preconditioning techniques for finite element and finite difference methods.
LFA <span id="id21">[<a class="reference internal" href="#id85" title="Achi Brandt. Multi-level adaptive solutions to boundary-value problems. Mathematics of Computation, 31(138):333–390, 1977.">Bra77</a>]</span> was originally developed in the context of analyzing <span class="math notranslate nohighlight">\(h\)</span>-multigrid methods for finite difference methods, but since then LFA has been used to analyze finite element methods and a variety of preconditioning techniques.
I wrote the Julia package <a class="reference external" href="https://www.github.com/jeremylt/LFAToolkit.jl">LFAToolkit.jl</a> <span id="id22">[<a class="reference internal" href="#id107" title="Jeremy L Thompson. LFAToolkit. https://github.com/jeremylt/LFAToolkit.jl, 2021.">Tho21</a>]</span>, a toolkit for analyzing the performance of preconditioners a priori for arbitrary, user provided weak forms of second order PDEs.
While this technique is not designed for complex meshes use in many research simulations, it offers good intuition on how preconditioners will perform on more complex meshes and offers a rigorous way to compare the performance of different preconditioners and parameter values for the same problem.</p>
<p>With appropriate parameter tuning, these preconditioning techniques can greatly improve the total iteration count and therefore the end to end runtime for scientific simulations in HONEE and Ratel.
This allows us to make better us of resources and complete more simulations with the same allocation of HPC resources.</p>
</section>
<section id="future-work">
<h2>Future Work<a class="headerlink" href="#future-work" title="Link to this heading">¶</a></h2>
<p>I thrive in large research projects, such as the Center for Efficient Exascale Discretizations (CEED) as part of the Exascale Computing Project (ECP) and the Center for Micromorphic Multiphysics Porous and Particulate Materials Simulations within Exascale Computing Workflows as part of the Predictive Science Academic Alliance Program (PSAAP), and have helped make connections between libCEED and larger open source software packages, such as PETSc, MFEM, and deal.II.
My work in libCEED is partially funded in the FASTMath Institute of the Scientific Discovery through Advanced Computing (SciDAC).
I am working to use the relationships I have established via this work to help identify funding for myself and graduate students.</p>
<p>I have contributed to the proposal for the renewal FASTMath for SciDAC-6 and intend to pursue further partnerships as part of the upcoming SciDAC applications call.
I have participated in a Small Business Innovation and Research (SBIR) proposal and anticipate future funding diversification will require a mix of government lab and industry partnerships.
As computational needs grow and hardware technology continues to advance, performance portable software will continue to be an important area of research with wide range of stakeholders.</p>
</section>
<section id="references">
<h2>References<a class="headerlink" href="#references" title="Link to this heading">¶</a></h2>
<div class="docutils container" id="id23">
<div role="list" class="citation-list">
<div class="citation" id="id108" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id10">CUD21</a><span class="fn-bracket">]</span></span>
<p>Cuda. 2021. URL: <a class="reference external" href="https://developer.nvidia.com/about-cuda">https://developer.nvidia.com/about-cuda</a>.</p>
</div>
<div class="citation" id="id109" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id11">HIP21</a><span class="fn-bracket">]</span></span>
<p>Hip. 2021. URL: <a class="reference external" href="https://rocmdocs.amd.com/en/latest/Programming_Guides/HIP-GUIDE.html">https://rocmdocs.amd.com/en/latest/Programming_Guides/HIP-GUIDE.html</a>.</p>
</div>
<div class="citation" id="id110" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id9">lib21</a><span class="fn-bracket">]</span></span>
<p>LIBXSMM. <a class="reference external" href="https://github.com/hfp/libxsmm">https://github.com/hfp/libxsmm</a>, 2021.</p>
</div>
<div class="citation" id="id102" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id8">ABB+21</a><span class="fn-bracket">]</span></span>
<p>Ahmad Abdelfattah, Valeria Barra, Natalie Beams, Jed Brown, Jean-Sylvain Camier, Veselin Dobrev, Yohann Dudouit, Leila Ghaffari, Tzanio Kolev, David Medina, Thilina Rathnayake, Jeremy L Thompson, and Stanimire Tomov. LibCEED User Manual. July 2021. URL: <a class="reference external" href="https://doi.org/10.5281/zenodo.4302737">https://doi.org/10.5281/zenodo.4302737</a>, <a class="reference external" href="https://doi.org/10.5281/zenodo.4302737">doi:10.5281/zenodo.4302737</a>.</p>
</div>
<div class="citation" id="id34" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id4">ABS+14</a><span class="fn-bracket">]</span></span>
<p>Mark Adams, Jed Brown, John Shalf, Brian Straalen, Erich Strohmaier, and Samuel Williams. HPGMG 1.0: a benchmark for ranking high performance computing systems. <em>LBNL Technical Report</em>, pages, 05 2014.</p>
</div>
<div class="citation" id="id100" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id6">BabuvskaS94</a><span class="fn-bracket">]</span></span>
<p>Ivo Babuška and Manil Suri. The p and h-p versions of the finite element method, basic principles and properties. <em>SIAM review</em>, 36(4):578–632, 1994.</p>
</div>
<div class="citation" id="id99" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id6">BabuvskaS82</a><span class="fn-bracket">]</span></span>
<p>Ivo Babuška and Barna Szabo. On the rates of convergence of the finite element method. <em>International Journal for Numerical Methods in Engineering</em>, 18(3):323–341, 1982.</p>
</div>
<div class="citation" id="id104" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id12">BAA+21</a><span class="fn-bracket">]</span></span>
<p>Satish Balay, Shrirang Abhyankar, Mark F Adams, Jed Brown, Peter Brune, Kris Buschelman, Lisandro Dalcin, Alp Dener, Victor Eijkhout, William D Gropp, Dmitry Karpeyev, Dinesh Kaushik, Matthew G Knepley, Dave A May, Lois Curfman McInnes, Richard Tran Mills, Todd Munson, Karl Rupp, Patrick Sanan, Barry F Smith, Stefano Zampini, Hong Zhang, and Hong Zhang. PETSc users manual. Technical Report ANL-95/11 - Revision 3.15, Argonne National Laboratory, 2021. URL: <a class="reference external" href="https://www.mcs.anl.gov/petsc">https://www.mcs.anl.gov/petsc</a>.</p>
</div>
<div class="citation" id="id85" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id21">Bra77</a><span class="fn-bracket">]</span></span>
<p>Achi Brandt. Multi-level adaptive solutions to boundary-value problems. <em>Mathematics of Computation</em>, 31(138):333–390, 1977.</p>
</div>
<div class="citation" id="id103" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id8">BAB+21</a><span class="fn-bracket">]</span></span>
<p>Jed Brown, Ahmad Abdelfattah, Valeria Barra, Natalie Beams, Jean Sylvain Camier, Veselin Dobrev, Yohann Dudouit, Leila Ghaffari, Tzanio Kolev, David Medina, Will Pazner, Thilina Ratnayaka, Jeremy Thompson, and Stan Tomov. libCEED: fast algebra for high-order element-based discretizations. <em>Journal of Open Source Software</em>, 6(63):2945, 2021. <a class="reference external" href="https://doi.org/10.21105/joss.02945">doi:10.21105/joss.02945</a>.</p>
</div>
<div class="citation" id="id117" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id15">CAB+20</a><span class="fn-bracket">]</span></span>
<p>William M. Coombs, Charles E. Augarde, Andrew J. Brennan, Michael J. Brown, Tim J. Charlton, Jonathan A. Knappett, Yousef Ghaffari Motlagh, and Lei Wang. On lagrangian mechanics and the implicit material point method for large deformation elasto-plasticity. <em>Computer Methods in Applied Mechanics and Engineering</em>, 358:112622, January 2020. <a class="reference external" href="https://doi.org/10.1016/j.cma.2019.112622">doi:10.1016/j.cma.2019.112622</a>.</p>
</div>
<div class="citation" id="id33" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id5">DHL16</a><span class="fn-bracket">]</span></span>
<p>Jack Dongarra, Michael A Heroux, and Piotr Luszczek. High-performance conjugate-gradient benchmark. <em>International Journal of High Performance Computing Applications</em>, 30(1):3–10, 2016.</p>
</div>
<div class="citation" id="id105" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id16">FLSjogreen+21</a><span class="fn-bracket">]</span></span>
<p>Robert D Falgout, Ruipeng Li, Björn Sjögreen, Lu Wang, and Ulrike Meier Yang. Porting hypre to heterogeneous computer architectures: strategies and experiences. <em>Parallel Computing</em>, 108:102840, 2021. <a class="reference external" href="https://doi.org/10.1016/j.parco.2021.102840">doi:10.1016/j.parco.2021.102840</a>.</p>
</div>
<div class="citation" id="id101" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id6">GBabuvska86</a><span class="fn-bracket">]</span></span>
<p>B Guo and Ivo Babuška. The hp version of the finite element method. <em>Computational Mechanics</em>, 1(1):21–41, 1986.</p>
</div>
<div class="citation" id="id47" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id19">HGK98</a><span class="fn-bracket">]</span></span>
<p>Ning Hu, Xian-Zhong Guo, and I Norman Katz. Bounds for eigenvalues and condition numbers in the p-version of the finite element method. <em>Mathematics of Computation</em>, 67(224):1423–1450, 1998.</p>
</div>
<div class="citation" id="id120" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id14">HST10</a><span class="fn-bracket">]</span></span>
<p>Thomas J R Hughes, Guglielmo Scovazzi, and Tayfun E Tezduyar. Stabilized methods for compressible flows. <em>Journal of Scientific Computing</em>, 43:343–368, 2010. <a class="reference external" href="https://doi.org/10.1007/s10915-008-9233-5">doi:10.1007/s10915-008-9233-5</a>.</p>
</div>
<div class="citation" id="id46" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id18">Lue73</a><span class="fn-bracket">]</span></span>
<p>David G Luenberger. <em>Introduction to linear and nonlinear programming</em>. Volume 28. Addison-Wesley Reading, MA, 1973.</p>
</div>
<div class="citation" id="id30" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id1">McC16</a><span class="fn-bracket">]</span></span>
<p>John D McCalpin. Memory bandwidth and system balance in HPC systems. <em>Invited talk, Supercomputing</em>, 2016.</p>
</div>
<div class="citation" id="id31" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id2">MSD+20</a><span class="fn-bracket">]</span></span>
<p>Hans Meuer, Erich Strohmaier, Jack Dongarra, Horst Simon, and Martin Meuer. Top 500 list. 2020. URL: <a class="reference external" href="http://www.top500.org/">http://www.top500.org/</a>.</p>
</div>
<div class="citation" id="id118" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id15">MDMuhlhaus03</a><span class="fn-bracket">]</span></span>
<p>Louis Moresi, Frédéric Dufour, and H-B Mühlhaus. A lagrangian integration point finite element method for large deformation modeling of viscoelastic geomaterials. <em>Journal of computational physics</em>, 184(2):476–497, 2003. <a class="reference external" href="https://doi.org/10.1016/S0021-9991(02)00031-1">doi:10.1016/S0021-9991(02)00031-1</a>.</p>
</div>
<div class="citation" id="id32" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id3">PWDC04</a><span class="fn-bracket">]</span></span>
<p>Antoine Petitet, R Clint Whaley, Jack Dongarra, and Andrew Cleary. HPL-a portable implementation of the high-performance Linpack benchmark for distributed-memory computers. 2004. URL: <a class="reference external" href="http://www.netlib.org/benchmark/hpl/">http://www.netlib.org/benchmark/hpl/</a>.</p>
</div>
<div class="citation" id="id51" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id20">RonquistP87</a><span class="fn-bracket">]</span></span>
<p>Einar M Rønquist and Anthony T Patera. Spectral element multigrid. I. formulation and numerical results. <em>Journal of Scientific Computing</em>, 2(4):389–406, 1987.</p>
</div>
<div class="citation" id="id43" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id7">SGTB24</a><span class="fn-bracket">]</span></span>
<p>Rezgar Shakeri, Leila Ghaffari, Jeremy Thompson, and Jed Brown. Stable numerics for finite-strain elasticity. <em>International Journal for Numerical Methods in Engineering</em>, pages e7563, 2024. URL: <a class="reference external" href="https://onlinelibrary.wiley.com/doi/abs/10.1002/nme.7563">https://onlinelibrary.wiley.com/doi/abs/10.1002/nme.7563</a>, <a class="reference external" href="https://arxiv.org/abs/https://onlinelibrary.wiley.com/doi/pdf/10.1002/nme.7563">arXiv:https://onlinelibrary.wiley.com/doi/pdf/10.1002/nme.7563</a>, <a class="reference external" href="https://doi.org/10.1002/nme.7563">doi:10.1002/nme.7563</a>.</p>
</div>
<div class="citation" id="id119" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id13">SHJ91</a><span class="fn-bracket">]</span></span>
<p>Farzin Shakib, Thomas JR Hughes, and Zdeněk Johan. A new finite element formulation for computational fluid dynamics: X. the compressible Euler and Navier-Stokes equations. <em>Computer Methods in Applied Mechanics and Engineering</em>, 89(1-3):141–219, 1991. <a class="reference external" href="https://doi.org/10.1016/0045-7825(91)90041-4">doi:10.1016/0045-7825(91)90041-4</a>.</p>
</div>
<div class="citation" id="id107" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id22">Tho21</a><span class="fn-bracket">]</span></span>
<p>Jeremy L Thompson. LFAToolkit. <a class="reference external" href="https://github.com/jeremylt/LFAToolkit.jl">https://github.com/jeremylt/LFAToolkit.jl</a>, 2021.</p>
</div>
<div class="citation" id="id106" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id17">TLGA+22</a><span class="fn-bracket">]</span></span>
<p>Christian R. Trott, Damien Lebrun-Grandié, Daniel Arndt, Jan Ciesko, Vinh Dang, Nathan Ellingwood, Rahulkumar Gayatri, Evan Harvey, Daisy S. Hollman, Dan Ibanez, Nevin Liber, Jonathan Madsen, Jeff Miles, David Poliakoff, Amy Powell, Sivasankaran Rajamanickam, Mikael Simberg, Dan Sunderland, Bruno Turcksin, and Jeremiah Wilke. Kokkos 3: programming model extensions for the exascale era. <em>IEEE Transactions on Parallel and Distributed Systems</em>, 33(4):805–817, 2022. <a class="reference external" href="https://doi.org/10.1109/TPDS.2021.3097283">doi:10.1109/TPDS.2021.3097283</a>.</p>
</div>
</div>
</div>
</section>
</section>


          </div>
          
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &#169;2025, Jeremy L Thompson.
      
      |
      Powered by <a href="https://www.sphinx-doc.org/">Sphinx 9.0.4</a>
      &amp; <a href="https://alabaster.readthedocs.io">Alabaster 1.0.0</a>
      
      |
      <a href="_sources/research.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>